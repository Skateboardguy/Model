{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c8000c-709c-4745-bc90-2fa20433f582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 分割資料 \u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split( X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my ) \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Step 1: 初始模型 (baseline) \u001b[39;00m\n\u001b[0;32m     33\u001b[0m xgb_clf \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier( objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m ) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.給我XGBoost範例，以台灣金融業鷹眼模型為例 \n",
    "2.並且就之前討論的調參順序，檢視模型正確率是否提升 \n",
    "3.你說的超參數重點： scale_pos_weight (關鍵） , min_child_weight, gamma \n",
    "4.接著整體的 max_depth learning_rate estimators\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#以下提供一個 以台灣金融業「鷹眼模型」風格的 XGBoost 範例，並依照你之前提到的調參順序（重點參數 → 整體參數），檢視模型正確率是否提升。\n",
    "\n",
    "'''\n",
    "\n",
    "1. 範例場景：台灣金融業鷹眼模型\n",
    "假設資料集為信用卡交易，目標是 偵測是否為可疑交易（fraud / 非fraud）。特徵包含：\n",
    "•\t客戶年齡、信用卡等級\n",
    "•\t交易金額、交易次數\n",
    "•\t是否跨境、是否高風險行業\n",
    "目標變數：is_fraud (1=詐欺，0=正常)。\n",
    "\n",
    "\n",
    "'''\n",
    "#2. XGBoost 建模流程 (Python)\n",
    "import xgboost as xgb \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 分割資料 \n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y ) \n",
    "\n",
    "# Step 1: 初始模型 (baseline) \n",
    "xgb_clf = xgb.XGBClassifier( objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=42, use_label_encoder=False ) \n",
    "xgb_clf.fit(X_train, y_train) \n",
    "y_pred = xgb_clf.predict(X_test) \n",
    "print(\"Baseline Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "'''\n",
    "#3. 調參順序與重點\n",
    "(A) 不平衡處理 — scale_pos_weight\n",
    "金融詐欺通常正負樣本極度不平衡。\n",
    "計算方式：\n",
    "'''\n",
    "\n",
    "scale = float(sum(y == 0)) / sum(y == 1)\n",
    "\n",
    "'''\n",
    "(B) 控制過擬合 — min_child_weight, gamma\n",
    "•\tmin_child_weight: 每個葉子節點最小樣本權重和，越大越保守\n",
    "•\tgamma: 分裂需要的最小 loss reduction，越大越保守\n",
    "(C) 整體結構 — max_depth, learning_rate, n_estimators\n",
    "•\tmax_depth: 控制樹深度\n",
    "•\tlearning_rate: shrinkage，通常 0.05~0.2\n",
    "•\tn_estimators: 樹的數量\n",
    "'''\n",
    "\n",
    "\n",
    "#4. GridSearch 調參範例\n",
    "param_grid = { \"scale_pos_weight\": [scale], \"min_child_weight\": [1, 3, 5],\"gamma\": [0, 0.1, 0.2, 0.5], \"max_depth\": [3, 5, 7], \"learning_rate\": [0.05, 0.1, 0.2], \"n_estimators\": [200, 400, 600] } \n",
    "\n",
    "grid = GridSearchCV( estimator=xgb.XGBClassifier( objective=\"binary:logistic\", eval_metric=\"auc\", random_state=42, use_label_encoder=False ),param_grid=param_grid, scoring=\"accuracy\", cv=3, verbose=1, n_jobs=-1 ) \n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_) \n",
    "print(\"Best CV Accuracy:\", grid.best_score_)\n",
    "\n",
    "# 測試集表現 \n",
    "y_pred_best = grid.best_estimator_.predict(X_test) \n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_best)) \n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "'''\n",
    "5. 分析正確率提升\n",
    "•\tBaseline 模型：未調參 → Accuracy 可能只有 0.70 左右\n",
    "•\t加入 scale_pos_weight：對抗樣本不平衡 → Recall 提升\n",
    "•\t加入 min_child_weight、gamma：減少過擬合 → Accuracy、F1 Score 提升\n",
    "•\t最後調整 max_depth、learning_rate、n_estimators → 模型更穩定，整體正確率可望提升到 0.80~0.85+\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GridSearchCV 的用途是 自動化超參數搜尋與交叉驗證。\n",
    "核心功能：\n",
    "1.\t遍歷參數組合\n",
    "o\t你提供一個參數網格 param_grid。\n",
    "o\t它會嘗試所有組合，例如 (min_child_weight=1, gamma=0.1, max_depth=5, …)。\n",
    "2.\t交叉驗證 (Cross Validation)\n",
    "o\t每組參數會跑 K 折 (例如 3-fold) 切分資料，避免單次切分的偶然性。\n",
    "o\t輸出平均表現（如 Accuracy、AUC）。\n",
    "3.\t找最佳模型\n",
    "o\t自動回傳表現最佳的參數組合 best_params_。\n",
    "o\tbest_estimator_ 直接就是最佳模型，可拿來預測測試集。\n",
    "與手動調參的差異\n",
    "•\t手動：一次改一個參數 → 慢\n",
    "•\tGridSearchCV：系統化遍歷所有可能組合 → 全面，但耗時\n",
    "若資料量大，可以改用 RandomizedSearchCV（隨機取樣部分組合），速度快。\n",
    "'''\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
